{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libaries and set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import io\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "test_file = '/home/george/Documents/george_vae/testing/Y9_44814.45096055_9_10_12_31_36.wav'\n",
    "all_audio_files = '/home/george/Documents/george_vae/testing/all_audio_files/' # path to all audio files\n",
    "root = '/home/george/Documents/george_vae/testing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the segments folder directory, extract all the files out of their day wise folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir('/home/george/Documents/george_vae/testing/segments/'):\n",
    "    # move every file from folder to the segments folder\n",
    "    for file in os.listdir('/home/george/Documents/george_vae/testing/segments/' + folder):\n",
    "        os.rename('/home/george/Documents/george_vae/testing/segments/' + folder + '/' + file, '/home/george/Documents/george_vae/testing/segments/' + str(folder) + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take files from the segment folder and move it into the testing and training folders, I actually think there might be a bug and the split between training and testing is not occuring correctly although its not a big deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data, validation, testing split\n",
    "train_p = .7\n",
    "test_p = .3\n",
    "\n",
    "# get the amount of files in segments\n",
    "# and split them into the three sets\n",
    "file_list = os.listdir('/home/george/Documents/george_vae/testing/segments')\n",
    "file_list = [file for file in file_list if file.endswith('.png')]\n",
    "\n",
    "# randomize the order of the files\n",
    "np.random.shuffle(file_list)\n",
    "\n",
    "file_list_len = len(file_list)\n",
    "\n",
    "num_train = file_list_len * train_p\n",
    "num_train = int(math.floor(num_train))\n",
    "\n",
    "num_validation = file_list_len * validation_p\n",
    "num_validation = int(math.floor(num_validation))\n",
    "\n",
    "num_test = file_list_len * test_p\n",
    "num_test = int(math.floor(num_test))\n",
    "\n",
    "# remove the rounding error files into the test category\n",
    "num_test += file_list_len - (num_train + num_validation + num_test)\n",
    "\n",
    "# move the num_train items from segments to root+ train \n",
    "for i in range(num_train):\n",
    "    os.rename('/home/george/Documents/george_vae/testing/segments/' + file_list[i], '/home/george/Documents/george_vae/testing/train/1/' + file_list[i])\n",
    "\n",
    "# move the num_validation items from segments to root+ validation\n",
    "for i in range(num_validation):\n",
    "    os.rename('/home/george/Documents/george_vae/testing/segments/' + file_list[i + num_train], '/home/george/Documents/george_vae/testing/validation/1/' + file_list[i + num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for neural net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 32\n",
    "num_epochs = 100 #usually 100\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "variational_beta = 1\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# pytorch, get the data from /train folder\n",
    "# and transform it into a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    # normalize\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=root+'train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=root+'test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the actual neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batchNorm1 = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, 2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8,16,3, 1, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(16)\n",
    "        self.conv4 = nn.Conv2d(16,16,3,2, padding=1)\n",
    "        self.batchNorm5 = nn.BatchNorm2d(16)\n",
    "        self.conv5 = nn.Conv2d(16,24,3,1, padding=1)\n",
    "        self.batchNorm6 = nn.BatchNorm2d(24)\n",
    "        self.conv6 = nn.Conv2d(24,24,3,2, padding=1)\n",
    "        self.batchNorm7 = nn.BatchNorm2d(24)\n",
    "        self.conv7 = nn.Conv2d(24,32,3,1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(in_features=64, out_features=latent_dims)\n",
    "        self.fc_logvar = nn.Linear(in_features=64, out_features=latent_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(self.batchNorm1(x)))\n",
    "        x = F.relu(self.conv2(self.batchNorm2(x)))\n",
    "        x = F.relu(self.conv3(self.batchNorm3(x)))\n",
    "        x = F.relu(self.conv4(self.batchNorm4(x)))\n",
    "        x = F.relu(self.conv5(self.batchNorm5(x)))\n",
    "        x = F.relu(self.conv6(self.batchNorm6(x)))\n",
    "        x = F.relu(self.conv7(self.batchNorm7(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x_mu = self.fc_mu(x)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=latent_dims, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=1024)\n",
    "        self.fc4 = nn.Linear(in_features=1024, out_features=8192)\n",
    "        self.unflatten = nn.Unflatten(1, (32, 16, 16))\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(32,24,3,1,padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(24,24,3,2,padding=1,output_padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(24,16,3,1,padding=1)\n",
    "        self.conv4 = nn.ConvTranspose2d(16,16,3,2,padding=1,output_padding=1)\n",
    "        self.conv5 = nn.ConvTranspose2d(16,8,3,1,padding=1)\n",
    "        self.conv6 = nn.ConvTranspose2d(8,8,3,2,padding=1,output_padding=1)\n",
    "        self.conv7 = nn.ConvTranspose2d(8,1,3,1,padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(24)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(24)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(16)\n",
    "        self.batchNorm5 = nn.BatchNorm2d(16)\n",
    "        self.batchNorm6 = nn.BatchNorm2d(8)\n",
    "        self.batchNorm7 = nn.BatchNorm2d(8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        x = F.relu(self.conv1(self.batchNorm1(x)))\n",
    "        x = F.relu(self.conv2(self.batchNorm2(x)))\n",
    "        x = F.relu(self.conv3(self.batchNorm3(x)))\n",
    "        x = F.relu(self.conv4(self.batchNorm4(x)))\n",
    "        x = F.relu(self.conv5(self.batchNorm5(x)))\n",
    "        x = F.relu(self.conv6(self.batchNorm6(x)))\n",
    "        x = F.relu(self.conv7(self.batchNorm7(x)))\n",
    "\n",
    "        x = F.relu(x) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "## the code below was heavily inspired by a VAE tutorial, but I can't find it and thus can't credit it :(\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
    "\n",
    "        x_recon = self.decoder(latent)\n",
    "\n",
    "        return x_recon, latent_mu, latent_logvar\n",
    "    \n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    flatten = nn.Flatten()\n",
    "    flattened_recon = flatten(recon_x)\n",
    "    flattened_x = flatten(x)\n",
    "    recon_loss = F.mse_loss(flattened_recon, flattened_x, reduction='sum')\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + kldivergence * variational_beta\n",
    "    \n",
    "    \n",
    "vae = VariationalAutoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "vae = vae.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# set to training mode\n",
    "vae.train()\n",
    "\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss_avg = []\n",
    "    validation_loss = []\n",
    "\n",
    "    for image_batch, _ in train_loader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "\n",
    "        # vae reconstruction\n",
    "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
    "\n",
    "        # reconstruction error\n",
    "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg.append(loss.item())\n",
    "\n",
    "    for image_batch, _ in test_loader:\n",
    "        image_batch = image_batch.to(device)\n",
    "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
    "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
    "        validation_loss.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(vae.state_dict(), '/home/george/Documents/george_vae/testing/training_checkpoints/vae_epoch_{}.pth'.format(epoch))\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f\"train loss: {np.mean(train_loss_avg)}\")\n",
    "    print(f\"validation loss: {np.mean(validation_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to load a previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torch model\n",
    "vae.load_state_dict(torch.load('/home/george/Documents/george_vae/testing/training_checkpoints/vae_epoch_20.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before and after reconstruction visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "vae.eval()\n",
    "\n",
    "# This function takes as an input the images to reconstruct\n",
    "# and the name of the model with which the reconstructions\n",
    "# are performed\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        images = images.to(device)\n",
    "        images, _, _ = model(images)\n",
    "        images = images.cpu()\n",
    "        images = to_img(images)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "images, labels = iter(test_loader).next()\n",
    "\n",
    "# First visualise the original images\n",
    "print('Original images')\n",
    "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
    "plt.show()\n",
    "\n",
    "# Reconstruct and visualise the images using the vae\n",
    "print('VAE reconstruction:')\n",
    "visualise_output(images, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next part of the notebook is concerned with creating UMAP representaions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# contains a list of batches which contain the latent space variables\n",
    "list_of_latent_averages = []\n",
    "list_of_images = []\n",
    "\n",
    "for image_batch, _ in train_loader:\n",
    "    image_batch = image_batch.to(device)\n",
    "\n",
    "    # returns tuple of average and std of latent space in that order\n",
    "    output = vae.encoder(image_batch)\n",
    "    output = output[0]\n",
    "\n",
    "    # for the visualization\n",
    "    image_batch.squeeze_(1)\n",
    "\n",
    "    # convert to numpy array\n",
    "    output = output.cpu()\n",
    "    output = output.detach().numpy()\n",
    "    image_batch = image_batch.cpu()\n",
    "    image_batch = image_batch.detach().numpy()\n",
    "\n",
    "    for latent_vector in output:\n",
    "        list_of_latent_averages.append(latent_vector)\n",
    "\n",
    "    for image in image_batch:\n",
    "        list_of_images.append(image)\n",
    "\n",
    "# convert list of latent averages to a numpy array\n",
    "list_of_latent_averages = np.array(list_of_latent_averages)\n",
    "list_of_images = np.array(list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=20, min_dist=0.1, n_components=2, metric='euclidean')\n",
    "reducer.fit(list_of_latent_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(list_of_latent_averages)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection ~230ms pre-transection data', fontsize=24);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "def embeddable_image(data):\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    # convert to uint8\n",
    "    data = np.uint8(data)\n",
    "    image = Image.fromarray(data)\n",
    "    image = image.convert('RGB')\n",
    "    # show PIL image\n",
    "    im_file = BytesIO()\n",
    "    img_save = image.save(im_file, format='PNG')\n",
    "    im_bytes = im_file.getvalue()\n",
    "\n",
    "    img_str = \"data:image/png;base64,\" + base64.b64encode(im_bytes).decode()\n",
    "    return img_str\n",
    "\n",
    "base64_str = embeddable_image(list_of_images[0])\n",
    "\n",
    "\n",
    "# save base64_str to a file\n",
    "with open('base64_str.txt', 'w') as f:\n",
    "    f.write(base64_str)\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_file(filename='umap.html')\n",
    "\n",
    "digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "digits_df['image'] = list(map(embeddable_image, list_of_images))\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP',\n",
    "    plot_width=600,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    fill_color = 'gray'\n",
    ")\n",
    "\n",
    "save(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the files that fall outside of the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# produces a list of files that are good for training\n",
    "# iterate through src folder, put each of the files through the VAE and get the latent space variables\n",
    "# then create an embedding through a reducer\n",
    "# if the values are within the range x1, x2, y1, y2 then add the file to a text file that contains the list of files to train on \n",
    "\n",
    "x1 = -4\n",
    "x2 = 12\n",
    "y1 = -10\n",
    "y2 = 6 \n",
    "\n",
    "imgs = '/home/george/Documents/george_vae/testing/DoubleDerivative/'\n",
    "\n",
    "# delete good_files.txt if it exists\n",
    "try:\n",
    "    os.remove('good_files.txt')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "array_of_latent_vectors = []\n",
    "array_of_names = []\n",
    "\n",
    "for folder in os.listdir(imgs):\n",
    "    for file in os.listdir(imgs + folder):\n",
    "        image = Image.open(imgs + folder + '/' + file)\n",
    "        image = TF.to_grayscale(image, num_output_channels=1)\n",
    "        image = TF.to_tensor(image)\n",
    "        # gray scale the image\n",
    "        image = image.to(device)\n",
    "        # make 32 copies of the image in a single batch dimension\n",
    "        image = image.unsqueeze(0).repeat(32, 1, 1, 1)\n",
    "\n",
    "        latent_vector = vae.encoder(image)\n",
    "        # get the mean of the latent vector\n",
    "        latent_vector = latent_vector[0]\n",
    "        # get the first image of the batch, the bottom and above statements are NOT redundant\n",
    "        latent_vector = latent_vector[0]\n",
    "        latent_vector = latent_vector.cpu()\n",
    "        latent_vector = latent_vector.detach().numpy()\n",
    "        array_of_latent_vectors.append(latent_vector)\n",
    "        array_of_names.append(str(folder + ':' + file))\n",
    "\n",
    "\n",
    "embeddings = reducer.transform(array_of_latent_vectors)\n",
    "\n",
    "for embedding in embeddings:\n",
    "    if embedding[0] > x1 and embedding[0] < x2 and embedding[1] > y1 and embedding[1] < y2:\n",
    "        # get the index of the embedding\n",
    "        index = np.where(embeddings == embedding)\n",
    "        # get the name of the image\n",
    "        name = array_of_names[index[0][0]]\n",
    "        # write the name to a file\n",
    "        with open('good_files.txt', 'a') as f:\n",
    "            f.write(name)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a GIF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchvision.transforms.functional as TF\n",
    "import datetime as dt \n",
    "\n",
    "dir = '/home/george/Documents/george_vae/testing/UMAP_Over_Time_Y11/'\n",
    "imageDir = '/home/george/Documents/george_vae/testing/UMAP_Over_Time_Images/'\n",
    "\n",
    "list_of_latent_vectors = []\n",
    "list_of_embeddings = []\n",
    "\n",
    "# set the start date to october 1st 2022\n",
    "start_date = dt.datetime(2022, 10, 11)\n",
    "\n",
    "for file in range(1, 37):\n",
    "    list_of_latent_vectors = []\n",
    "\n",
    "    dir = '/home/george/Documents/george_vae/testing/UMAP_Over_Time_Y11/' + str(file) + '/'\n",
    "    for x in os.listdir(dir):\n",
    "        image = Image.open(dir + str(x))\n",
    "        image = TF.to_grayscale(image, num_output_channels=1)\n",
    "        image = TF.to_tensor(image)\n",
    "        # gray scale the image\n",
    "        image = image.to(device)\n",
    "        # make 32 copies of the image in a single batch dimension\n",
    "        image = image.unsqueeze(0).repeat(32, 1, 1, 1)\n",
    "\n",
    "        latent_vector = vae.encoder(image)\n",
    "        # get the mean of the latent vector\n",
    "        latent_vector = latent_vector[0]\n",
    "        # get the first image of the batch, the bottom and above statements are NOT redundant\n",
    "        latent_vector = latent_vector[0]\n",
    "        latent_vector = latent_vector.cpu()\n",
    "        latent_vector = latent_vector.detach().numpy()\n",
    "        list_of_latent_vectors.append(latent_vector)\n",
    "\n",
    "    # convert list of latent averages to a numpy array\n",
    "    list_of_latent_vectors = np.array(list_of_latent_vectors)\n",
    "    embedding = reducer.transform(list_of_latent_vectors)\n",
    "    list_of_embeddings.append(embedding)\n",
    "\n",
    "\n",
    "    if file < 10:\n",
    "        plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=5, color='blue')\n",
    "    else:\n",
    "        plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=5, color='red')\n",
    "\n",
    "    # add a title, start_date to string\n",
    "    plt.title('date:' + str(start_date.date()), fontsize=24)\n",
    "\n",
    "    # iterate date by one day \n",
    "    start_date = start_date + dt.timedelta(days=1)\n",
    "\n",
    "    # plt.scatter(before_embedding[:, 0], before_embedding[:, 1], cmap='Spectral', s=5, color='red')\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    # set axis from -10 to 15 both x and y \n",
    "    plt.xlim(-10, 25)\n",
    "    plt.ylim(-10, 25)\n",
    "    # save\n",
    "    plt.savefig(imageDir + str(file) + 'UMAP.png')\n",
    "    # reset plt\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates an interactive plot for selective dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchvision.transforms.functional as TF\n",
    "import datetime as dt \n",
    "\n",
    "UMAP_file_num = 7\n",
    "\n",
    "dir = '/home/george/Documents/george_vae/testing/UMAP_Over_Time_Y11/' + str(UMAP_file_num) + '/'\n",
    "\n",
    "list_of_images = []\n",
    "list_of_latent_vectors = []\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    # read the image\n",
    "    image = Image.open(dir + str(file))\n",
    "    # convert to grayscale\n",
    "    image = Image.open(dir + str(file))\n",
    "    image = TF.to_grayscale(image, num_output_channels=1)\n",
    "    image = TF.to_tensor(image)\n",
    "    # gray scale the image\n",
    "    # squeeze the image\n",
    "    x = image.squeeze(0)\n",
    "    x = np.array(x)\n",
    "    list_of_images.append(x)\n",
    "\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0).repeat(32, 1, 1, 1)\n",
    "    latent_vector = vae.encoder(image)\n",
    "    # get the mean of the latent vector\n",
    "    latent_vector = latent_vector[0]\n",
    "    # get the first image of the batch, the bottom and above statements are NOT redundant\n",
    "    latent_vector = latent_vector[0]\n",
    "    latent_vector = latent_vector.cpu()\n",
    "    latent_vector = latent_vector.detach().numpy()\n",
    "    list_of_latent_vectors.append(latent_vector)\n",
    "\n",
    "# convert list of latent averages to a numpy array\n",
    "list_of_latent_vectors = np.array(list_of_latent_vectors)\n",
    "list_of_images = np.array(list_of_images)\n",
    "embedding = reducer.transform(list_of_latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "def embeddable_image(data):\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    # convert to uint8\n",
    "    data = np.uint8(data)\n",
    "    image = Image.fromarray(data)\n",
    "    image = image.convert('RGB')\n",
    "    # show PIL image\n",
    "    im_file = BytesIO()\n",
    "    img_save = image.save(im_file, format='PNG')\n",
    "    im_bytes = im_file.getvalue()\n",
    "\n",
    "    img_str = \"data:image/png;base64,\" + base64.b64encode(im_bytes).decode()\n",
    "    return img_str\n",
    "\n",
    "# base64_str = embeddable_image(list_of_images[0])\n",
    "\n",
    "\n",
    "# # save base64_str to a file\n",
    "# with open('base64_str.txt', 'w') as f:\n",
    "#     f.write(base64_str)\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_file(filename='umap' + str(UMAP_file_num) + '.html')\n",
    "\n",
    "digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "digits_df['image'] = list(map(embeddable_image, list_of_images))\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP',\n",
    "    plot_width=600,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    fill_color = 'gray'\n",
    ")\n",
    "\n",
    "save(plot_figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ddfca89fb0ff7038606e5fd782b59d233ff8cbd5b2a6aabb29749a82eca2773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
